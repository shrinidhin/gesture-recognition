{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread, imresize\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(50)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('val.csv').readlines())\n",
    "batch_size = 30 #experiment with the batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    #create a list of image numbers you want to use for a particular video:Note that here,assuming that consecutive frames will not vary much in detail, we have skipped one frame between two frames for each image.\n",
    "    img_idx = list(range(1,30,2))\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = int(len(folder_list) / batch_size)# calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            x = len(img_idx)\n",
    "            y = 120\n",
    "            z = 120\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    image = imresize(image,(120,120), interp=\"bicubic\")\n",
    "                    batch_data[folder,idx] = image\n",
    "                    \n",
    "                    R = batch_data[folder,idx,:,:,0]#normalise and feed in the image\n",
    "                    R = (R - np.mean(R)) / (np.max(R) - np.min(R))\n",
    "                    batch_data[folder,idx,:,:,0] = R\n",
    "                    \n",
    "                    G = batch_data[folder,idx,:,:,1]#normalise and feed in the image\n",
    "                    G = (G - np.mean(G)) / (np.max(G) - np.min(G))\n",
    "                    batch_data[folder,idx,:,:,1] = G\n",
    "                    \n",
    "                    B = batch_data[folder,idx,:,:,2]#normalise and feed in the image\n",
    "                    B = (B - np.mean(B)) / (np.max(B) - np.min(B))\n",
    "                    batch_data[folder,idx,:,:,2] = B\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        remaining_batch = (len(folder_list) - (num_batches * batch_size))\n",
    "        if (len(folder_list) % batch_size) != 0:\n",
    "            x = len(img_idx)\n",
    "            y = 120\n",
    "            z = 120\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3))\n",
    "            batch_labels = np.zeros((batch_size,5))\n",
    "            for folder in range(remaining_batch): # iterate over the batch_size\n",
    "                    imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                    for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                        image = imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "\n",
    "                        #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                        #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                        image = imresize(image,(120,120), interp=\"bicubic\")\n",
    "                        batch_data[folder,idx] = image\n",
    "                        R = batch_data[folder,idx,:,:,0]#normalise and feed in the image\n",
    "                        R = (R - np.mean(R)) / (np.max(R) - np.min(R))\n",
    "                        batch_data[folder,idx,:,:,0] = R\n",
    "\n",
    "                        G = batch_data[folder,idx,:,:,1]#normalise and feed in the image\n",
    "                        G = (G - np.mean(G)) / (np.max(G) - np.min(G))\n",
    "                        batch_data[folder,idx,:,:,1] = G\n",
    "\n",
    "                        B = batch_data[folder,idx,:,:,2]#normalise and feed in the image\n",
    "                        B = (B - np.mean(B)) / (np.max(B) - np.min(B))\n",
    "                        batch_data[folder,idx,:,:,2] = B\n",
    "\n",
    "                    batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 150\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = './train'\n",
    "val_path = './val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 150# choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (15,120,120,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D-Conv Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D, Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "\n",
    "#write your model here\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(8,(3,3,3), activation='relu',padding=\"same\", input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv3D(16,(3,3,3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size = (2,2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(32,(3,3,3), activation='relu', padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv3D(32,(3,3,3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size = (2,2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(64,(3,3,3), activation='relu', padding=\"same\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size = (2,2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, kernel_regularizer=l2(0.1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_36 (Conv3D)           (None, 15, 120, 120, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_136 (Bat (None, 15, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "conv3d_37 (Conv3D)           (None, 13, 118, 118, 16)  3472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_137 (Bat (None, 13, 118, 118, 16)  64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_22 (MaxPooling (None, 6, 59, 59, 16)     0         \n",
      "_________________________________________________________________\n",
      "dropout_118 (Dropout)        (None, 6, 59, 59, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_38 (Conv3D)           (None, 6, 59, 59, 32)     13856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_138 (Bat (None, 6, 59, 59, 32)     128       \n",
      "_________________________________________________________________\n",
      "conv3d_39 (Conv3D)           (None, 4, 57, 57, 32)     27680     \n",
      "_________________________________________________________________\n",
      "batch_normalization_139 (Bat (None, 4, 57, 57, 32)     128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_23 (MaxPooling (None, 2, 28, 28, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_119 (Dropout)        (None, 2, 28, 28, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_40 (Conv3D)           (None, 2, 28, 28, 64)     55360     \n",
      "_________________________________________________________________\n",
      "batch_normalization_140 (Bat (None, 2, 28, 28, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_24 (MaxPooling (None, 1, 14, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_120 (Dropout)        (None, 1, 14, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_48 (Flatten)         (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 128)               1605760   \n",
      "_________________________________________________________________\n",
      "activation_115 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_121 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_116 (Activation)  (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,708,037\n",
      "Trainable params: 1,707,733\n",
      "Non-trainable params: 304\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = 'adam'\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init_3D_Conv' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001) # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Source path =  ./val ; batch size = 30\n",
      "Source path =  ./train ; batch size = 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\wbt-gamelauncher\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  app.launch_new_instance()\n",
      "c:\\users\\wbt-gamelauncher\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:20: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3/23 [==>...........................] - ETA: 1:50 - loss: 33.9825 - categorical_accuracy: 0.1778"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\wbt-gamelauncher\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:49: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "c:\\users\\wbt-gamelauncher\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:53: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 102s 4s/step - loss: 30.1510 - categorical_accuracy: 0.2203 - val_loss: 22.9327 - val_categorical_accuracy: 0.2333\n",
      "\n",
      "Epoch 00001: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00001-30.15099-0.22029-22.93266-0.23333.h5\n",
      "Epoch 2/150\n",
      "23/23 [==============================] - 84s 4s/step - loss: 18.3064 - categorical_accuracy: 0.3565 - val_loss: 16.9222 - val_categorical_accuracy: 0.2500\n",
      "\n",
      "Epoch 00002: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00002-18.30644-0.35652-16.92216-0.25000.h5\n",
      "Epoch 3/150\n",
      "23/23 [==============================] - 89s 4s/step - loss: 10.7525 - categorical_accuracy: 0.4710 - val_loss: 11.1996 - val_categorical_accuracy: 0.3083\n",
      "\n",
      "Epoch 00003: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00003-10.75250-0.47101-11.19960-0.30833.h5\n",
      "Epoch 4/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 6.8454 - categorical_accuracy: 0.5275 - val_loss: 7.2609 - val_categorical_accuracy: 0.3583\n",
      "\n",
      "Epoch 00004: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00004-6.84539-0.52754-7.26088-0.35833.h5\n",
      "Epoch 5/150\n",
      "23/23 [==============================] - 92s 4s/step - loss: 5.2676 - categorical_accuracy: 0.5739 - val_loss: 4.5308 - val_categorical_accuracy: 0.4750\n",
      "\n",
      "Epoch 00005: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00005-5.26760-0.57391-4.53081-0.47500.h5\n",
      "Epoch 6/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 3.8807 - categorical_accuracy: 0.6522 - val_loss: 3.4468 - val_categorical_accuracy: 0.5500\n",
      "\n",
      "Epoch 00006: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00006-3.88070-0.65217-3.44682-0.55000.h5\n",
      "Epoch 7/150\n",
      "23/23 [==============================] - 89s 4s/step - loss: 2.7562 - categorical_accuracy: 0.7565 - val_loss: 2.6596 - val_categorical_accuracy: 0.5250\n",
      "\n",
      "Epoch 00007: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00007-2.75618-0.75652-2.65963-0.52500.h5\n",
      "Epoch 8/150\n",
      "23/23 [==============================] - 90s 4s/step - loss: 2.2088 - categorical_accuracy: 0.7406 - val_loss: 2.3086 - val_categorical_accuracy: 0.5583\n",
      "\n",
      "Epoch 00008: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00008-2.20882-0.74058-2.30858-0.55833.h5\n",
      "Epoch 9/150\n",
      "23/23 [==============================] - 90s 4s/step - loss: 2.0497 - categorical_accuracy: 0.7725 - val_loss: 1.8839 - val_categorical_accuracy: 0.6083\n",
      "\n",
      "Epoch 00009: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00009-2.04969-0.77246-1.88386-0.60833.h5\n",
      "Epoch 10/150\n",
      "23/23 [==============================] - 90s 4s/step - loss: 1.6291 - categorical_accuracy: 0.7928 - val_loss: 1.7582 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00010: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00010-1.62906-0.79275-1.75820-0.62500.h5\n",
      "Epoch 11/150\n",
      "23/23 [==============================] - 89s 4s/step - loss: 1.3777 - categorical_accuracy: 0.8000 - val_loss: 1.8105 - val_categorical_accuracy: 0.5667\n",
      "\n",
      "Epoch 00011: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00011-1.37768-0.80000-1.81046-0.56667.h5\n",
      "Epoch 12/150\n",
      "23/23 [==============================] - 87s 4s/step - loss: 1.4161 - categorical_accuracy: 0.8101 - val_loss: 1.7288 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00012: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00012-1.41610-0.81014-1.72882-0.62500.h5\n",
      "Epoch 13/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 1.6564 - categorical_accuracy: 0.8116 - val_loss: 1.9765 - val_categorical_accuracy: 0.6167\n",
      "\n",
      "Epoch 00013: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00013-1.65645-0.81159-1.97653-0.61667.h5\n",
      "Epoch 14/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 1.8097 - categorical_accuracy: 0.8203 - val_loss: 1.9253 - val_categorical_accuracy: 0.8250\n",
      "\n",
      "Epoch 00014: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00014-1.80967-0.82029-1.92529-0.82500.h5\n",
      "Epoch 15/150\n",
      "23/23 [==============================] - 89s 4s/step - loss: 1.7565 - categorical_accuracy: 0.8145 - val_loss: 1.7988 - val_categorical_accuracy: 0.6667\n",
      "\n",
      "Epoch 00015: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00015-1.75645-0.81449-1.79876-0.66667.h5\n",
      "Epoch 16/150\n",
      "23/23 [==============================] - 90s 4s/step - loss: 1.6431 - categorical_accuracy: 0.8188 - val_loss: 1.8245 - val_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00016: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00016-1.64305-0.81884-1.82447-0.67500.h5\n",
      "Epoch 17/150\n",
      "23/23 [==============================] - 90s 4s/step - loss: 1.5315 - categorical_accuracy: 0.8435 - val_loss: 1.7301 - val_categorical_accuracy: 0.6333\n",
      "\n",
      "Epoch 00017: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00017-1.53147-0.84348-1.73011-0.63333.h5\n",
      "Epoch 18/150\n",
      "23/23 [==============================] - 87s 4s/step - loss: 1.3921 - categorical_accuracy: 0.8609 - val_loss: 1.5337 - val_categorical_accuracy: 0.6333\n",
      "\n",
      "Epoch 00018: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00018-1.39210-0.86087-1.53365-0.63333.h5\n",
      "Epoch 19/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 1.2320 - categorical_accuracy: 0.8507 - val_loss: 1.2437 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00019: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00019-1.23204-0.85072-1.24368-0.62500.h5\n",
      "Epoch 20/150\n",
      "23/23 [==============================] - 89s 4s/step - loss: 0.9988 - categorical_accuracy: 0.8913 - val_loss: 1.2325 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00020: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00020-0.99876-0.89130-1.23254-0.65000.h5\n",
      "Epoch 21/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 1.2906 - categorical_accuracy: 0.8536 - val_loss: 1.4056 - val_categorical_accuracy: 0.7417\n",
      "\n",
      "Epoch 00021: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00021-1.29058-0.85362-1.40556-0.74167.h5\n",
      "Epoch 22/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 1.2921 - categorical_accuracy: 0.8623 - val_loss: 1.2957 - val_categorical_accuracy: 0.7083\n",
      "\n",
      "Epoch 00022: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00022-1.29207-0.86232-1.29573-0.70833.h5\n",
      "Epoch 23/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 1.3166 - categorical_accuracy: 0.8507 - val_loss: 1.5806 - val_categorical_accuracy: 0.6167\n",
      "\n",
      "Epoch 00023: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00023-1.31663-0.85072-1.58056-0.61667.h5\n",
      "Epoch 24/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 1.1889 - categorical_accuracy: 0.8899 - val_loss: 1.3482 - val_categorical_accuracy: 0.6917\n",
      "\n",
      "Epoch 00024: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00024-1.18888-0.88986-1.34819-0.69167.h5\n",
      "Epoch 25/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.9786 - categorical_accuracy: 0.8710 - val_loss: 1.0455 - val_categorical_accuracy: 0.7083\n",
      "\n",
      "Epoch 00025: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00025-0.97857-0.87101-1.04547-0.70833.h5\n",
      "Epoch 26/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.9640 - categorical_accuracy: 0.8812 - val_loss: 1.2881 - val_categorical_accuracy: 0.6667\n",
      "\n",
      "Epoch 00026: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00026-0.96396-0.88116-1.28810-0.66667.h5\n",
      "Epoch 27/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 1.0281 - categorical_accuracy: 0.8783 - val_loss: 1.3109 - val_categorical_accuracy: 0.6917\n",
      "\n",
      "Epoch 00027: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00027-1.02807-0.87826-1.31087-0.69167.h5\n",
      "Epoch 28/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 1.1747 - categorical_accuracy: 0.8884 - val_loss: 1.5222 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00028: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00028-1.17468-0.88841-1.52223-0.65000.h5\n",
      "Epoch 29/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 1.4812 - categorical_accuracy: 0.8406 - val_loss: 1.9735 - val_categorical_accuracy: 0.6750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00029: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00029-1.48118-0.84058-1.97349-0.67500.h5\n",
      "Epoch 30/150\n",
      "23/23 [==============================] - 87s 4s/step - loss: 1.5906 - categorical_accuracy: 0.8696 - val_loss: 1.6551 - val_categorical_accuracy: 0.7083\n",
      "\n",
      "Epoch 00030: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00030-1.59057-0.86957-1.65510-0.70833.h5\n",
      "Epoch 31/150\n",
      "23/23 [==============================] - 89s 4s/step - loss: 1.1677 - categorical_accuracy: 0.9087 - val_loss: 0.9117 - val_categorical_accuracy: 0.9250\n",
      "\n",
      "Epoch 00031: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00031-1.16772-0.90870-0.91166-0.92500.h5\n",
      "Epoch 32/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.7791 - categorical_accuracy: 0.9246 - val_loss: 0.7986 - val_categorical_accuracy: 0.9333\n",
      "\n",
      "Epoch 00032: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00032-0.77915-0.92464-0.79857-0.93333.h5\n",
      "Epoch 33/150\n",
      "23/23 [==============================] - 89s 4s/step - loss: 0.6489 - categorical_accuracy: 0.9261 - val_loss: 0.9531 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00033: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00033-0.64893-0.92609-0.95310-0.70000.h5\n",
      "Epoch 34/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.6746 - categorical_accuracy: 0.9101 - val_loss: 0.9616 - val_categorical_accuracy: 0.7250\n",
      "\n",
      "Epoch 00034: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00034-0.67457-0.91014-0.96157-0.72500.h5\n",
      "Epoch 35/150\n",
      "23/23 [==============================] - 91s 4s/step - loss: 0.5939 - categorical_accuracy: 0.9377 - val_loss: 0.7777 - val_categorical_accuracy: 0.7333\n",
      "\n",
      "Epoch 00035: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00035-0.59394-0.93768-0.77772-0.73333.h5\n",
      "Epoch 36/150\n",
      "23/23 [==============================] - 87s 4s/step - loss: 0.4239 - categorical_accuracy: 0.9406 - val_loss: 0.6009 - val_categorical_accuracy: 0.9083\n",
      "\n",
      "Epoch 00036: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00036-0.42391-0.94058-0.60090-0.90833.h5\n",
      "Epoch 37/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.4521 - categorical_accuracy: 0.9333 - val_loss: 0.6380 - val_categorical_accuracy: 0.8667\n",
      "\n",
      "Epoch 00037: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00037-0.45209-0.93333-0.63795-0.86667.h5\n",
      "Epoch 38/150\n",
      "23/23 [==============================] - 87s 4s/step - loss: 0.5462 - categorical_accuracy: 0.9304 - val_loss: 0.7981 - val_categorical_accuracy: 0.7250\n",
      "\n",
      "Epoch 00038: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00038-0.54619-0.93043-0.79812-0.72500.h5\n",
      "Epoch 39/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.5867 - categorical_accuracy: 0.9203 - val_loss: 0.6657 - val_categorical_accuracy: 0.7917\n",
      "\n",
      "Epoch 00039: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00039-0.58668-0.92029-0.66570-0.79167.h5\n",
      "Epoch 40/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.5861 - categorical_accuracy: 0.9246 - val_loss: 0.7601 - val_categorical_accuracy: 0.8750\n",
      "\n",
      "Epoch 00040: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00040-0.58609-0.92464-0.76012-0.87500.h5\n",
      "Epoch 41/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.8663 - categorical_accuracy: 0.9101 - val_loss: 1.0648 - val_categorical_accuracy: 0.7833\n",
      "\n",
      "Epoch 00041: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00041-0.86632-0.91014-1.06481-0.78333.h5\n",
      "Epoch 42/150\n",
      "23/23 [==============================] - 89s 4s/step - loss: 1.1207 - categorical_accuracy: 0.9058 - val_loss: 1.4059 - val_categorical_accuracy: 0.7250\n",
      "\n",
      "Epoch 00042: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00042-1.12073-0.90580-1.40585-0.72500.h5\n",
      "Epoch 43/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.8890 - categorical_accuracy: 0.9246 - val_loss: 0.9551 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00043: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00043-0.88903-0.92464-0.95512-0.75000.h5\n",
      "Epoch 44/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.6000 - categorical_accuracy: 0.9362 - val_loss: 0.7034 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00044: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00044-0.59995-0.93623-0.70343-0.76667.h5\n",
      "Epoch 45/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.4534 - categorical_accuracy: 0.9478 - val_loss: 0.5619 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00045: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00045-0.45337-0.94783-0.56186-0.77500.h5\n",
      "Epoch 46/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.6036 - categorical_accuracy: 0.9101 - val_loss: 0.9833 - val_categorical_accuracy: 0.7333\n",
      "\n",
      "Epoch 00046: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00046-0.60363-0.91014-0.98335-0.73333.h5\n",
      "Epoch 47/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 1.1979 - categorical_accuracy: 0.8812 - val_loss: 1.8073 - val_categorical_accuracy: 0.8667\n",
      "\n",
      "Epoch 00047: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00047-1.19793-0.88116-1.80727-0.86667.h5\n",
      "Epoch 48/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 1.9797 - categorical_accuracy: 0.8507 - val_loss: 2.2448 - val_categorical_accuracy: 0.6917\n",
      "\n",
      "Epoch 00048: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00048-1.97970-0.85072-2.24478-0.69167.h5\n",
      "Epoch 49/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 1.5664 - categorical_accuracy: 0.8855 - val_loss: 1.4349 - val_categorical_accuracy: 0.6917\n",
      "\n",
      "Epoch 00049: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00049-1.56638-0.88551-1.43493-0.69167.h5\n",
      "Epoch 50/150\n",
      "23/23 [==============================] - 89s 4s/step - loss: 0.8694 - categorical_accuracy: 0.9246 - val_loss: 0.7577 - val_categorical_accuracy: 0.9417\n",
      "\n",
      "Epoch 00050: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00050-0.86941-0.92464-0.75773-0.94167.h5\n",
      "Epoch 51/150\n",
      "23/23 [==============================] - 89s 4s/step - loss: 0.4983 - categorical_accuracy: 0.9449 - val_loss: 0.6011 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00051: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00051-0.49833-0.94493-0.60105-0.76667.h5\n",
      "Epoch 52/150\n",
      "23/23 [==============================] - 87s 4s/step - loss: 0.3511 - categorical_accuracy: 0.9464 - val_loss: 0.4863 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00052: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00052-0.35113-0.94638-0.48632-0.77500.h5\n",
      "Epoch 53/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.2735 - categorical_accuracy: 0.9507 - val_loss: 0.3692 - val_categorical_accuracy: 0.7833\n",
      "\n",
      "Epoch 00053: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00053-0.27351-0.95072-0.36919-0.78333.h5\n",
      "Epoch 54/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.3960 - categorical_accuracy: 0.9246 - val_loss: 0.5927 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00054: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00054-0.39596-0.92464-0.59268-0.77500.h5\n",
      "Epoch 55/150\n",
      "23/23 [==============================] - 87s 4s/step - loss: 0.8870 - categorical_accuracy: 0.9000 - val_loss: 0.9738 - val_categorical_accuracy: 0.9583\n",
      "\n",
      "Epoch 00055: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00055-0.88695-0.90000-0.97380-0.95833.h5\n",
      "Epoch 56/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 1.0818 - categorical_accuracy: 0.8971 - val_loss: 1.0661 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00056: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00056-1.08179-0.89710-1.06614-0.75000.h5\n",
      "Epoch 57/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.9690 - categorical_accuracy: 0.9304 - val_loss: 0.9529 - val_categorical_accuracy: 0.9417\n",
      "\n",
      "Epoch 00057: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00057-0.96903-0.93043-0.95286-0.94167.h5\n",
      "Epoch 58/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 88s 4s/step - loss: 0.6873 - categorical_accuracy: 0.9420 - val_loss: 0.5457 - val_categorical_accuracy: 0.9833\n",
      "\n",
      "Epoch 00058: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00058-0.68735-0.94203-0.54568-0.98333.h5\n",
      "Epoch 59/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.8027 - categorical_accuracy: 0.9029 - val_loss: 0.9889 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00059: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00059-0.80269-0.90290-0.98888-0.76667.h5\n",
      "Epoch 60/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.9695 - categorical_accuracy: 0.9072 - val_loss: 1.1649 - val_categorical_accuracy: 0.7333\n",
      "\n",
      "Epoch 00060: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00060-0.96946-0.90725-1.16494-0.73333.h5\n",
      "Epoch 61/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.7439 - categorical_accuracy: 0.9391 - val_loss: 0.6980 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00061: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00061-0.74393-0.93913-0.69804-0.76667.h5\n",
      "Epoch 62/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.4992 - categorical_accuracy: 0.9406 - val_loss: 0.5283 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00062: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00062-0.49919-0.94058-0.52833-0.77500.h5\n",
      "Epoch 63/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.9114 - categorical_accuracy: 0.9159 - val_loss: 1.4172 - val_categorical_accuracy: 0.7417\n",
      "\n",
      "Epoch 00063: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00063-0.91138-0.91594-1.41718-0.74167.h5\n",
      "Epoch 64/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 1.1773 - categorical_accuracy: 0.9072 - val_loss: 1.1425 - val_categorical_accuracy: 0.7583\n",
      "\n",
      "Epoch 00064: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00064-1.17732-0.90725-1.14254-0.75833.h5\n",
      "Epoch 65/150\n",
      "23/23 [==============================] - 87s 4s/step - loss: 1.0692 - categorical_accuracy: 0.9159 - val_loss: 1.1646 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00065: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00065-1.06916-0.91594-1.16462-0.76667.h5\n",
      "Epoch 66/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 1.4361 - categorical_accuracy: 0.8928 - val_loss: 1.8916 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00066: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00066-1.43605-0.89275-1.89159-0.70000.h5\n",
      "Epoch 67/150\n",
      "23/23 [==============================] - 89s 4s/step - loss: 1.2595 - categorical_accuracy: 0.9145 - val_loss: 1.1656 - val_categorical_accuracy: 0.7250\n",
      "\n",
      "Epoch 00067: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00067-1.25948-0.91449-1.16564-0.72500.h5\n",
      "Epoch 68/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.9571 - categorical_accuracy: 0.9232 - val_loss: 0.8551 - val_categorical_accuracy: 0.7833\n",
      "\n",
      "Epoch 00068: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00068-0.95711-0.92319-0.85510-0.78333.h5\n",
      "Epoch 69/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.8915 - categorical_accuracy: 0.9246 - val_loss: 1.1240 - val_categorical_accuracy: 0.7250\n",
      "\n",
      "Epoch 00069: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00069-0.89153-0.92464-1.12400-0.72500.h5\n",
      "Epoch 70/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.8764 - categorical_accuracy: 0.9203 - val_loss: 0.8003 - val_categorical_accuracy: 0.7917\n",
      "\n",
      "Epoch 00070: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00070-0.87636-0.92029-0.80028-0.79167.h5\n",
      "Epoch 71/150\n",
      "23/23 [==============================] - 87s 4s/step - loss: 0.7274 - categorical_accuracy: 0.9377 - val_loss: 0.7184 - val_categorical_accuracy: 0.7833\n",
      "\n",
      "Epoch 00071: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00071-0.72741-0.93768-0.71839-0.78333.h5\n",
      "Epoch 72/150\n",
      "23/23 [==============================] - 87s 4s/step - loss: 0.5368 - categorical_accuracy: 0.9464 - val_loss: 0.5399 - val_categorical_accuracy: 0.7917\n",
      "\n",
      "Epoch 00072: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00072-0.53677-0.94638-0.53991-0.79167.h5\n",
      "Epoch 73/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.5834 - categorical_accuracy: 0.9391 - val_loss: 0.6401 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00073: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00073-0.58340-0.93913-0.64009-0.80000.h5\n",
      "Epoch 74/150\n",
      "23/23 [==============================] - 87s 4s/step - loss: 0.4787 - categorical_accuracy: 0.9464 - val_loss: 0.3947 - val_categorical_accuracy: 0.8083\n",
      "\n",
      "Epoch 00074: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00074-0.47867-0.94638-0.39471-0.80833.h5\n",
      "Epoch 75/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.3033 - categorical_accuracy: 0.9464 - val_loss: 0.3482 - val_categorical_accuracy: 0.9667\n",
      "\n",
      "Epoch 00075: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00075-0.30330-0.94638-0.34816-0.96667.h5\n",
      "Epoch 76/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.2495 - categorical_accuracy: 0.9536 - val_loss: 0.2536 - val_categorical_accuracy: 0.8167\n",
      "\n",
      "Epoch 00076: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00076-0.24945-0.95362-0.25359-0.81667.h5\n",
      "Epoch 77/150\n",
      "23/23 [==============================] - 87s 4s/step - loss: 0.1923 - categorical_accuracy: 0.9551 - val_loss: 0.2479 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00077: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00077-0.19234-0.95507-0.24790-0.80000.h5\n",
      "Epoch 78/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.2019 - categorical_accuracy: 0.9536 - val_loss: 0.2622 - val_categorical_accuracy: 0.7917\n",
      "\n",
      "Epoch 00078: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00078-0.20193-0.95362-0.26216-0.79167.h5\n",
      "Epoch 79/150\n",
      "23/23 [==============================] - 87s 4s/step - loss: 0.7070 - categorical_accuracy: 0.9232 - val_loss: 1.1151 - val_categorical_accuracy: 0.7333\n",
      "\n",
      "Epoch 00079: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00079-0.70697-0.92319-1.11505-0.73333.h5\n",
      "Epoch 80/150\n",
      "23/23 [==============================] - 87s 4s/step - loss: 0.8732 - categorical_accuracy: 0.9333 - val_loss: 0.7703 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00080: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00080-0.87318-0.93333-0.77032-0.76667.h5\n",
      "Epoch 81/150\n",
      "23/23 [==============================] - 86s 4s/step - loss: 0.7520 - categorical_accuracy: 0.9377 - val_loss: 0.7605 - val_categorical_accuracy: 0.7917\n",
      "\n",
      "Epoch 00081: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00081-0.75197-0.93768-0.76045-0.79167.h5\n",
      "Epoch 82/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.6740 - categorical_accuracy: 0.9377 - val_loss: 0.7644 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00082: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00082-0.67402-0.93768-0.76441-0.75000.h5\n",
      "Epoch 83/150\n",
      "23/23 [==============================] - 94s 4s/step - loss: 0.6203 - categorical_accuracy: 0.9362 - val_loss: 0.6279 - val_categorical_accuracy: 0.7833\n",
      "\n",
      "Epoch 00083: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00083-0.62031-0.93623-0.62795-0.78333.h5\n",
      "Epoch 84/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.4832 - categorical_accuracy: 0.9478 - val_loss: 0.4729 - val_categorical_accuracy: 0.7917\n",
      "\n",
      "Epoch 00084: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00084-0.48321-0.94783-0.47288-0.79167.h5\n",
      "Epoch 85/150\n",
      "23/23 [==============================] - 89s 4s/step - loss: 0.2961 - categorical_accuracy: 0.9565 - val_loss: 0.2779 - val_categorical_accuracy: 0.8083\n",
      "\n",
      "Epoch 00085: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00085-0.29607-0.95652-0.27786-0.80833.h5\n",
      "Epoch 86/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.2048 - categorical_accuracy: 0.9594 - val_loss: 0.2067 - val_categorical_accuracy: 0.8167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00086: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00086-0.20476-0.95942-0.20669-0.81667.h5\n",
      "Epoch 87/150\n",
      "23/23 [==============================] - 89s 4s/step - loss: 0.2947 - categorical_accuracy: 0.9536 - val_loss: 0.3461 - val_categorical_accuracy: 0.8333\n",
      "\n",
      "Epoch 00087: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00087-0.29473-0.95362-0.34615-0.83333.h5\n",
      "Epoch 88/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.3460 - categorical_accuracy: 0.9493 - val_loss: 0.4106 - val_categorical_accuracy: 0.7917\n",
      "\n",
      "Epoch 00088: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00088-0.34603-0.94928-0.41060-0.79167.h5\n",
      "Epoch 89/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.5719 - categorical_accuracy: 0.9391 - val_loss: 0.8805 - val_categorical_accuracy: 0.7417\n",
      "\n",
      "Epoch 00089: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00089-0.57193-0.93913-0.88052-0.74167.h5\n",
      "Epoch 90/150\n",
      "23/23 [==============================] - 85s 4s/step - loss: 0.6031 - categorical_accuracy: 0.9464 - val_loss: 0.6420 - val_categorical_accuracy: 0.7917\n",
      "\n",
      "Epoch 00090: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00090-0.60307-0.94638-0.64202-0.79167.h5\n",
      "Epoch 91/150\n",
      "23/23 [==============================] - 87s 4s/step - loss: 0.3734 - categorical_accuracy: 0.9536 - val_loss: 0.3915 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00091: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00091-0.37336-0.95362-0.39145-0.76667.h5\n",
      "Epoch 92/150\n",
      "23/23 [==============================] - 87s 4s/step - loss: 0.5290 - categorical_accuracy: 0.9812 - val_loss: 0.9677 - val_categorical_accuracy: 0.9083\n",
      "\n",
      "Epoch 00092: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00092-0.52902-0.98116-0.96767-0.90833.h5\n",
      "Epoch 93/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.8572 - categorical_accuracy: 0.9377 - val_loss: 1.1175 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00093: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00093-0.85717-0.93768-1.11748-0.75000.h5\n",
      "Epoch 94/150\n",
      "23/23 [==============================] - 92s 4s/step - loss: 0.6885 - categorical_accuracy: 0.9464 - val_loss: 0.5891 - val_categorical_accuracy: 0.7833\n",
      "\n",
      "Epoch 00094: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00094-0.68850-0.94638-0.58914-0.78333.h5\n",
      "Epoch 95/150\n",
      "23/23 [==============================] - 89s 4s/step - loss: 1.0369 - categorical_accuracy: 0.9217 - val_loss: 1.3690 - val_categorical_accuracy: 0.7917\n",
      "\n",
      "Epoch 00095: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00095-1.03687-0.92174-1.36898-0.79167.h5\n",
      "Epoch 96/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 1.0809 - categorical_accuracy: 0.9348 - val_loss: 1.1143 - val_categorical_accuracy: 0.7417\n",
      "\n",
      "Epoch 00096: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00096-1.08093-0.93478-1.11430-0.74167.h5\n",
      "Epoch 97/150\n",
      "23/23 [==============================] - 89s 4s/step - loss: 0.6224 - categorical_accuracy: 0.9493 - val_loss: 0.6226 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00097: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00097-0.62243-0.94928-0.62255-0.77500.h5\n",
      "Epoch 98/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.7612 - categorical_accuracy: 0.9638 - val_loss: 0.9725 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00098: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00098-0.76118-0.96377-0.97249-0.76667.h5\n",
      "Epoch 99/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 1.0453 - categorical_accuracy: 0.9348 - val_loss: 1.2424 - val_categorical_accuracy: 0.7083\n",
      "\n",
      "Epoch 00099: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00099-1.04531-0.93478-1.24237-0.70833.h5\n",
      "Epoch 100/150\n",
      "23/23 [==============================] - 87s 4s/step - loss: 0.9685 - categorical_accuracy: 0.9362 - val_loss: 0.9480 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00100: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00100-0.96852-0.93623-0.94800-0.75000.h5\n",
      "Epoch 101/150\n",
      "23/23 [==============================] - 87s 4s/step - loss: 0.6157 - categorical_accuracy: 0.9435 - val_loss: 0.5951 - val_categorical_accuracy: 0.7583\n",
      "\n",
      "Epoch 00101: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00101-0.61571-0.94348-0.59514-0.75833.h5\n",
      "Epoch 102/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.3803 - categorical_accuracy: 0.9522 - val_loss: 0.3854 - val_categorical_accuracy: 0.7917\n",
      "\n",
      "Epoch 00102: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00102-0.38026-0.95217-0.38544-0.79167.h5\n",
      "Epoch 103/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.2399 - categorical_accuracy: 0.9580 - val_loss: 0.2664 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00103: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00103-0.23994-0.95797-0.26644-0.80000.h5\n",
      "Epoch 104/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.4959 - categorical_accuracy: 0.9435 - val_loss: 0.9004 - val_categorical_accuracy: 0.9167\n",
      "\n",
      "Epoch 00104: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00104-0.49587-0.94348-0.90045-0.91667.h5\n",
      "Epoch 105/150\n",
      "23/23 [==============================] - 89s 4s/step - loss: 0.6387 - categorical_accuracy: 0.9348 - val_loss: 0.8323 - val_categorical_accuracy: 0.7583\n",
      "\n",
      "Epoch 00105: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00105-0.63874-0.93478-0.83229-0.75833.h5\n",
      "Epoch 106/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.5500 - categorical_accuracy: 0.9435 - val_loss: 0.6641 - val_categorical_accuracy: 0.7583\n",
      "\n",
      "Epoch 00106: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00106-0.54998-0.94348-0.66406-0.75833.h5\n",
      "Epoch 107/150\n",
      "23/23 [==============================] - 87s 4s/step - loss: 0.4963 - categorical_accuracy: 0.9391 - val_loss: 0.5800 - val_categorical_accuracy: 0.7583\n",
      "\n",
      "Epoch 00107: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00107-0.49631-0.93913-0.57996-0.75833.h5\n",
      "Epoch 108/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.9301 - categorical_accuracy: 0.9754 - val_loss: 1.1926 - val_categorical_accuracy: 0.9333\n",
      "\n",
      "Epoch 00108: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00108-0.93012-0.97536-1.19256-0.93333.h5\n",
      "Epoch 109/150\n",
      "23/23 [==============================] - 87s 4s/step - loss: 0.7830 - categorical_accuracy: 0.9913 - val_loss: 0.8553 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00109: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00109-0.78300-0.99130-0.85529-0.75000.h5\n",
      "Epoch 110/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.4534 - categorical_accuracy: 0.9870 - val_loss: 0.7343 - val_categorical_accuracy: 0.9000\n",
      "\n",
      "Epoch 00110: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00110-0.45343-0.98696-0.73429-0.90000.h5\n",
      "Epoch 111/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.5636 - categorical_accuracy: 0.9391 - val_loss: 0.7971 - val_categorical_accuracy: 0.7833\n",
      "\n",
      "Epoch 00111: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00111-0.56364-0.93913-0.79710-0.78333.h5\n",
      "Epoch 112/150\n",
      "23/23 [==============================] - 89s 4s/step - loss: 0.5331 - categorical_accuracy: 0.9464 - val_loss: 0.6260 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00112: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00112-0.53307-0.94638-0.62601-0.77500.h5\n",
      "Epoch 113/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.6086 - categorical_accuracy: 0.9493 - val_loss: 0.8565 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00113: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00113-0.60861-0.94928-0.85654-0.75000.h5\n",
      "Epoch 114/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.5015 - categorical_accuracy: 0.9522 - val_loss: 0.5324 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00114: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00114-0.50146-0.95217-0.53241-0.76667.h5\n",
      "Epoch 115/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 87s 4s/step - loss: 0.3087 - categorical_accuracy: 0.9551 - val_loss: 0.3509 - val_categorical_accuracy: 0.7917\n",
      "\n",
      "Epoch 00115: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00115-0.30873-0.95507-0.35088-0.79167.h5\n",
      "Epoch 116/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.4906 - categorical_accuracy: 0.9725 - val_loss: 0.6747 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00116: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00116-0.49062-0.97246-0.67466-0.80000.h5\n",
      "Epoch 117/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.5797 - categorical_accuracy: 0.9536 - val_loss: 0.5156 - val_categorical_accuracy: 0.8083\n",
      "\n",
      "Epoch 00117: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00117-0.57973-0.95362-0.51564-0.80833.h5\n",
      "Epoch 118/150\n",
      "23/23 [==============================] - 89s 4s/step - loss: 0.4547 - categorical_accuracy: 0.9565 - val_loss: 0.6227 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00118: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00118-0.45467-0.95652-0.62269-0.76667.h5\n",
      "Epoch 119/150\n",
      "23/23 [==============================] - 89s 4s/step - loss: 0.3826 - categorical_accuracy: 0.9507 - val_loss: 0.4548 - val_categorical_accuracy: 0.7917\n",
      "\n",
      "Epoch 00119: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00119-0.38259-0.95072-0.45484-0.79167.h5\n",
      "Epoch 120/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.3938 - categorical_accuracy: 0.9478 - val_loss: 0.5495 - val_categorical_accuracy: 0.9500\n",
      "\n",
      "Epoch 00120: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00120-0.39382-0.94783-0.54955-0.95000.h5\n",
      "Epoch 121/150\n",
      "23/23 [==============================] - 89s 4s/step - loss: 0.3279 - categorical_accuracy: 0.9594 - val_loss: 0.3764 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00121: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00121-0.32786-0.95942-0.37645-0.77500.h5\n",
      "Epoch 122/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.1577 - categorical_accuracy: 0.9594 - val_loss: 0.1936 - val_categorical_accuracy: 0.8083\n",
      "\n",
      "Epoch 00122: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00122-0.15773-0.95942-0.19359-0.80833.h5\n",
      "Epoch 123/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.1829 - categorical_accuracy: 0.9551 - val_loss: 0.2941 - val_categorical_accuracy: 0.8083\n",
      "\n",
      "Epoch 00123: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00123-0.18285-0.95507-0.29411-0.80833.h5\n",
      "Epoch 124/150\n",
      "23/23 [==============================] - 89s 4s/step - loss: 0.2948 - categorical_accuracy: 0.9478 - val_loss: 0.4191 - val_categorical_accuracy: 0.8083\n",
      "\n",
      "Epoch 00124: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00124-0.29480-0.94783-0.41912-0.80833.h5\n",
      "Epoch 125/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.3564 - categorical_accuracy: 0.9565 - val_loss: 0.3224 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00125: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00125-0.35638-0.95652-0.32235-0.80000.h5\n",
      "Epoch 126/150\n",
      "23/23 [==============================] - 89s 4s/step - loss: 0.2078 - categorical_accuracy: 0.9609 - val_loss: 0.2565 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00126: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00126-0.20777-0.96087-0.25653-0.80000.h5\n",
      "Epoch 127/150\n",
      "23/23 [==============================] - 90s 4s/step - loss: 0.2397 - categorical_accuracy: 0.9478 - val_loss: 0.6059 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00127: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00127-0.23970-0.94783-0.60592-0.77500.h5\n",
      "Epoch 128/150\n",
      "23/23 [==============================] - 89s 4s/step - loss: 0.3783 - categorical_accuracy: 0.9536 - val_loss: 0.9484 - val_categorical_accuracy: 0.7250\n",
      "\n",
      "Epoch 00128: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00128-0.37829-0.95362-0.94843-0.72500.h5\n",
      "Epoch 129/150\n",
      "23/23 [==============================] - 89s 4s/step - loss: 0.9230 - categorical_accuracy: 0.9072 - val_loss: 1.4053 - val_categorical_accuracy: 0.7417\n",
      "\n",
      "Epoch 00129: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00129-0.92297-0.90725-1.40532-0.74167.h5\n",
      "Epoch 130/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 1.2134 - categorical_accuracy: 0.9232 - val_loss: 1.1206 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00130: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00130-1.21343-0.92319-1.12060-0.77500.h5\n",
      "Epoch 131/150\n",
      "23/23 [==============================] - 89s 4s/step - loss: 0.9174 - categorical_accuracy: 0.9478 - val_loss: 0.8601 - val_categorical_accuracy: 0.7583\n",
      "\n",
      "Epoch 00131: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00131-0.91742-0.94783-0.86007-0.75833.h5\n",
      "Epoch 132/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.6064 - categorical_accuracy: 0.9710 - val_loss: 0.6369 - val_categorical_accuracy: 0.9417\n",
      "\n",
      "Epoch 00132: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00132-0.60638-0.97101-0.63693-0.94167.h5\n",
      "Epoch 133/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.7845 - categorical_accuracy: 0.9493 - val_loss: 1.0732 - val_categorical_accuracy: 0.9083\n",
      "\n",
      "Epoch 00133: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00133-0.78450-0.94928-1.07322-0.90833.h5\n",
      "Epoch 134/150\n",
      "23/23 [==============================] - 90s 4s/step - loss: 0.7538 - categorical_accuracy: 0.9493 - val_loss: 0.8281 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00134: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00134-0.75384-0.94928-0.82810-0.76667.h5\n",
      "Epoch 135/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.5344 - categorical_accuracy: 0.9493 - val_loss: 0.9777 - val_categorical_accuracy: 0.8667\n",
      "\n",
      "Epoch 00135: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00135-0.53443-0.94928-0.97767-0.86667.h5\n",
      "Epoch 136/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.4841 - categorical_accuracy: 0.9391 - val_loss: 0.7168 - val_categorical_accuracy: 0.7833\n",
      "\n",
      "Epoch 00136: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00136-0.48409-0.93913-0.71684-0.78333.h5\n",
      "Epoch 137/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.5330 - categorical_accuracy: 0.9507 - val_loss: 0.6944 - val_categorical_accuracy: 0.7917\n",
      "\n",
      "Epoch 00137: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00137-0.53301-0.95072-0.69435-0.79167.h5\n",
      "Epoch 138/150\n",
      "23/23 [==============================] - 90s 4s/step - loss: 0.4993 - categorical_accuracy: 0.9406 - val_loss: 0.6417 - val_categorical_accuracy: 0.7583\n",
      "\n",
      "Epoch 00138: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00138-0.49935-0.94058-0.64166-0.75833.h5\n",
      "Epoch 139/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.7190 - categorical_accuracy: 0.9362 - val_loss: 0.9277 - val_categorical_accuracy: 0.7917\n",
      "\n",
      "Epoch 00139: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00139-0.71901-0.93623-0.92768-0.79167.h5\n",
      "Epoch 140/150\n",
      "23/23 [==============================] - 87s 4s/step - loss: 0.7517 - categorical_accuracy: 0.9783 - val_loss: 0.8749 - val_categorical_accuracy: 0.9250\n",
      "\n",
      "Epoch 00140: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00140-0.75174-0.97826-0.87491-0.92500.h5\n",
      "Epoch 141/150\n",
      "23/23 [==============================] - 87s 4s/step - loss: 0.5033 - categorical_accuracy: 0.9783 - val_loss: 0.5106 - val_categorical_accuracy: 0.9583\n",
      "\n",
      "Epoch 00141: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00141-0.50331-0.97826-0.51063-0.95833.h5\n",
      "Epoch 142/150\n",
      "23/23 [==============================] - 87s 4s/step - loss: 0.3271 - categorical_accuracy: 0.9507 - val_loss: 0.4374 - val_categorical_accuracy: 0.7583\n",
      "\n",
      "Epoch 00142: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00142-0.32709-0.95072-0.43737-0.75833.h5\n",
      "Epoch 143/150\n",
      "23/23 [==============================] - 89s 4s/step - loss: 0.4511 - categorical_accuracy: 0.9826 - val_loss: 0.7352 - val_categorical_accuracy: 0.9167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00143: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00143-0.45108-0.98261-0.73519-0.91667.h5\n",
      "Epoch 144/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.3923 - categorical_accuracy: 0.9812 - val_loss: 0.3958 - val_categorical_accuracy: 0.9417\n",
      "\n",
      "Epoch 00144: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00144-0.39232-0.98116-0.39578-0.94167.h5\n",
      "Epoch 145/150\n",
      "23/23 [==============================] - 90s 4s/step - loss: 0.2294 - categorical_accuracy: 0.9667 - val_loss: 0.2743 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00145: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00145-0.22940-0.96667-0.27434-0.80000.h5\n",
      "Epoch 146/150\n",
      "23/23 [==============================] - 87s 4s/step - loss: 0.1471 - categorical_accuracy: 0.9884 - val_loss: 0.2073 - val_categorical_accuracy: 0.9583\n",
      "\n",
      "Epoch 00146: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00146-0.14710-0.98841-0.20733-0.95833.h5\n",
      "Epoch 147/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.0987 - categorical_accuracy: 0.9725 - val_loss: 0.1608 - val_categorical_accuracy: 0.9833\n",
      "\n",
      "Epoch 00147: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00147-0.09870-0.97246-0.16079-0.98333.h5\n",
      "Epoch 148/150\n",
      "23/23 [==============================] - 89s 4s/step - loss: 0.0742 - categorical_accuracy: 0.9609 - val_loss: 0.1526 - val_categorical_accuracy: 0.9750\n",
      "\n",
      "Epoch 00148: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00148-0.07423-0.96087-0.15257-0.97500.h5\n",
      "Epoch 149/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.0599 - categorical_accuracy: 0.9609 - val_loss: 0.1406 - val_categorical_accuracy: 0.8167\n",
      "\n",
      "Epoch 00149: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00149-0.05987-0.96087-0.14060-0.81667.h5\n",
      "Epoch 150/150\n",
      "23/23 [==============================] - 88s 4s/step - loss: 0.0673 - categorical_accuracy: 0.9609 - val_loss: 0.2223 - val_categorical_accuracy: 0.9500\n",
      "\n",
      "Epoch 00150: saving model to model_init_3D_Conv_2019-03-1518_12_18.422945/model-00150-0.06730-0.96087-0.22229-0.95000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a0d77d50b8>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Got best Accuracy at epoch: 147\n",
    "#### Train Accuracy: 0.9725\n",
    "#### Validation Accuracy: 0.9833"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN - RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\wbt-gamelauncher\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:30: UserWarning: Update your `GRU` call to the Keras 2 API: `GRU(return_sequences=True, kernel_regularizer=<keras.reg..., units=128)`\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(8, (3, 3), padding='same'), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(TimeDistributed(Conv2D(8, (3, 3))))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(16, (3, 3), padding='same')))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(TimeDistributed(Conv2D(32, (3, 3))))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same')))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(TimeDistributed(Dense(128, kernel_regularizer=l2(0.1))))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(GRU(output_dim=128, return_sequences=True, kernel_regularizer=l2(0.1)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeating steps executed for 3D-Conv Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_231 (TimeDi (None, 15, 120, 120, 8)   224       \n",
      "_________________________________________________________________\n",
      "activation_132 (Activation)  (None, 15, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_156 (Bat (None, 15, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "time_distributed_232 (TimeDi (None, 15, 118, 118, 8)   584       \n",
      "_________________________________________________________________\n",
      "activation_133 (Activation)  (None, 15, 118, 118, 8)   0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_157 (Bat (None, 15, 118, 118, 8)   32        \n",
      "_________________________________________________________________\n",
      "time_distributed_233 (TimeDi (None, 15, 59, 59, 8)     0         \n",
      "_________________________________________________________________\n",
      "dropout_137 (Dropout)        (None, 15, 59, 59, 8)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_234 (TimeDi (None, 15, 59, 59, 16)    1168      \n",
      "_________________________________________________________________\n",
      "activation_134 (Activation)  (None, 15, 59, 59, 16)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_158 (Bat (None, 15, 59, 59, 16)    64        \n",
      "_________________________________________________________________\n",
      "time_distributed_235 (TimeDi (None, 15, 57, 57, 32)    4640      \n",
      "_________________________________________________________________\n",
      "activation_135 (Activation)  (None, 15, 57, 57, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_159 (Bat (None, 15, 57, 57, 32)    128       \n",
      "_________________________________________________________________\n",
      "time_distributed_236 (TimeDi (None, 15, 28, 28, 32)    0         \n",
      "_________________________________________________________________\n",
      "dropout_138 (Dropout)        (None, 15, 28, 28, 32)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_237 (TimeDi (None, 15, 28, 28, 64)    18496     \n",
      "_________________________________________________________________\n",
      "activation_136 (Activation)  (None, 15, 28, 28, 64)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_160 (Bat (None, 15, 28, 28, 64)    256       \n",
      "_________________________________________________________________\n",
      "time_distributed_238 (TimeDi (None, 15, 14, 14, 64)    0         \n",
      "_________________________________________________________________\n",
      "dropout_139 (Dropout)        (None, 15, 14, 14, 64)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_239 (TimeDi (None, 15, 12544)         0         \n",
      "_________________________________________________________________\n",
      "time_distributed_240 (TimeDi (None, 15, 128)           1605760   \n",
      "_________________________________________________________________\n",
      "dropout_140 (Dropout)        (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "gru_24 (GRU)                 (None, 15, 128)           98688     \n",
      "_________________________________________________________________\n",
      "dropout_141 (Dropout)        (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_56 (Flatten)         (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 5)                 9605      \n",
      "=================================================================\n",
      "Total params: 1,739,677\n",
      "Trainable params: 1,739,421\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init_CNN_RNN' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001) # write the REducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Source path =  Source path =  ./train ; batch size =./val 30 ; batch size = 30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\wbt-gamelauncher\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  app.launch_new_instance()\n",
      "c:\\users\\wbt-gamelauncher\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:20: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3/23 [==>...........................] - ETA: 2:16 - loss: 46.4578 - categorical_accuracy: 0.1222"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\wbt-gamelauncher\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:49: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "c:\\users\\wbt-gamelauncher\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:53: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\n",
      "Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 122s 5s/step - loss: 36.4483 - categorical_accuracy: 0.2391 - val_loss: 24.1902 - val_categorical_accuracy: 0.3583\n",
      "\n",
      "Epoch 00001: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00001-36.44831-0.23913-24.19020-0.35833.h5\n",
      "Epoch 2/150\n",
      "23/23 [==============================] - 95s 4s/step - loss: 17.5753 - categorical_accuracy: 0.3551 - val_loss: 11.9192 - val_categorical_accuracy: 0.3667\n",
      "\n",
      "Epoch 00002: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00002-17.57525-0.35507-11.91921-0.36667.h5\n",
      "Epoch 3/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 9.7550 - categorical_accuracy: 0.4971 - val_loss: 8.0568 - val_categorical_accuracy: 0.4833\n",
      "\n",
      "Epoch 00003: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00003-9.75500-0.49710-8.05677-0.48333.h5\n",
      "Epoch 4/150\n",
      "23/23 [==============================] - 101s 4s/step - loss: 6.9321 - categorical_accuracy: 0.6203 - val_loss: 6.5451 - val_categorical_accuracy: 0.4833\n",
      "\n",
      "Epoch 00004: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00004-6.93207-0.62029-6.54512-0.48333.h5\n",
      "Epoch 5/150\n",
      "23/23 [==============================] - 114s 5s/step - loss: 5.5070 - categorical_accuracy: 0.6986 - val_loss: 5.2504 - val_categorical_accuracy: 0.5000\n",
      "\n",
      "Epoch 00005: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00005-5.50698-0.69855-5.25037-0.50000.h5\n",
      "Epoch 6/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 4.5176 - categorical_accuracy: 0.7116 - val_loss: 4.1756 - val_categorical_accuracy: 0.5750\n",
      "\n",
      "Epoch 00006: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00006-4.51761-0.71159-4.17558-0.57500.h5\n",
      "Epoch 7/150\n",
      "23/23 [==============================] - 102s 4s/step - loss: 3.5478 - categorical_accuracy: 0.7870 - val_loss: 3.3578 - val_categorical_accuracy: 0.5583\n",
      "\n",
      "Epoch 00007: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00007-3.54779-0.78696-3.35780-0.55833.h5\n",
      "Epoch 8/150\n",
      "23/23 [==============================] - 101s 4s/step - loss: 2.7876 - categorical_accuracy: 0.8087 - val_loss: 2.9331 - val_categorical_accuracy: 0.6000\n",
      "\n",
      "Epoch 00008: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00008-2.78759-0.80870-2.93311-0.60000.h5\n",
      "Epoch 9/150\n",
      "23/23 [==============================] - 106s 5s/step - loss: 2.5356 - categorical_accuracy: 0.7942 - val_loss: 2.4247 - val_categorical_accuracy: 0.6667\n",
      "\n",
      "Epoch 00009: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00009-2.53562-0.79420-2.42466-0.66667.h5\n",
      "Epoch 10/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 1.9691 - categorical_accuracy: 0.8594 - val_loss: 2.0325 - val_categorical_accuracy: 0.5750\n",
      "\n",
      "Epoch 00010: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00010-1.96910-0.85942-2.03254-0.57500.h5\n",
      "Epoch 11/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 1.6206 - categorical_accuracy: 0.8652 - val_loss: 1.6852 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00011: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00011-1.62056-0.86522-1.68518-0.65000.h5\n",
      "Epoch 12/150\n",
      "23/23 [==============================] - 102s 4s/step - loss: 1.5072 - categorical_accuracy: 0.8435 - val_loss: 1.6777 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00012: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00012-1.50722-0.84348-1.67767-0.62500.h5\n",
      "Epoch 13/150\n",
      "23/23 [==============================] - 102s 4s/step - loss: 1.4933 - categorical_accuracy: 0.8435 - val_loss: 1.6624 - val_categorical_accuracy: 0.6417\n",
      "\n",
      "Epoch 00013: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00013-1.49326-0.84348-1.66245-0.64167.h5\n",
      "Epoch 14/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 1.2465 - categorical_accuracy: 0.8594 - val_loss: 1.4217 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00014: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00014-1.24651-0.85942-1.42170-0.62500.h5\n",
      "Epoch 15/150\n",
      "23/23 [==============================] - 105s 5s/step - loss: 1.1205 - categorical_accuracy: 0.8768 - val_loss: 1.4207 - val_categorical_accuracy: 0.7583\n",
      "\n",
      "Epoch 00015: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00015-1.12048-0.87681-1.42069-0.75833.h5\n",
      "Epoch 16/150\n",
      "23/23 [==============================] - 105s 5s/step - loss: 0.9530 - categorical_accuracy: 0.9029 - val_loss: 1.2226 - val_categorical_accuracy: 0.6667\n",
      "\n",
      "Epoch 00016: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00016-0.95297-0.90290-1.22255-0.66667.h5\n",
      "Epoch 17/150\n",
      "23/23 [==============================] - 102s 4s/step - loss: 1.0639 - categorical_accuracy: 0.8565 - val_loss: 1.3306 - val_categorical_accuracy: 0.6333\n",
      "\n",
      "Epoch 00017: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00017-1.06386-0.85652-1.33059-0.63333.h5\n",
      "Epoch 18/150\n",
      "23/23 [==============================] - 102s 4s/step - loss: 0.9299 - categorical_accuracy: 0.8841 - val_loss: 0.9221 - val_categorical_accuracy: 0.7167\n",
      "\n",
      "Epoch 00018: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00018-0.92987-0.88406-0.92212-0.71667.h5\n",
      "Epoch 19/150\n",
      "23/23 [==============================] - 101s 4s/step - loss: 0.7962 - categorical_accuracy: 0.9043 - val_loss: 1.0099 - val_categorical_accuracy: 0.6583\n",
      "\n",
      "Epoch 00019: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00019-0.79622-0.90435-1.00988-0.65833.h5\n",
      "Epoch 20/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.8068 - categorical_accuracy: 0.9058 - val_loss: 1.0788 - val_categorical_accuracy: 0.6167\n",
      "\n",
      "Epoch 00020: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00020-0.80685-0.90580-1.07884-0.61667.h5\n",
      "Epoch 21/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 0.7636 - categorical_accuracy: 0.8971 - val_loss: 0.8455 - val_categorical_accuracy: 0.7333\n",
      "\n",
      "Epoch 00021: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00021-0.76365-0.89710-0.84554-0.73333.h5\n",
      "Epoch 22/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.6429 - categorical_accuracy: 0.9275 - val_loss: 0.8041 - val_categorical_accuracy: 0.6917\n",
      "\n",
      "Epoch 00022: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00022-0.64286-0.92754-0.80415-0.69167.h5\n",
      "Epoch 23/150\n",
      "23/23 [==============================] - 102s 4s/step - loss: 0.5183 - categorical_accuracy: 0.9246 - val_loss: 1.1054 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00023: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00023-0.51833-0.92464-1.10538-0.62500.h5\n",
      "Epoch 24/150\n",
      "23/23 [==============================] - 102s 4s/step - loss: 0.7701 - categorical_accuracy: 0.8870 - val_loss: 1.2941 - val_categorical_accuracy: 0.6333\n",
      "\n",
      "Epoch 00024: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00024-0.77013-0.88696-1.29410-0.63333.h5\n",
      "Epoch 25/150\n",
      "23/23 [==============================] - 102s 4s/step - loss: 0.6936 - categorical_accuracy: 0.9058 - val_loss: 1.0958 - val_categorical_accuracy: 0.6083\n",
      "\n",
      "Epoch 00025: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00025-0.69363-0.90580-1.09583-0.60833.h5\n",
      "Epoch 26/150\n",
      "23/23 [==============================] - 101s 4s/step - loss: 0.6377 - categorical_accuracy: 0.9072 - val_loss: 0.9338 - val_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00026: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00026-0.63769-0.90725-0.93382-0.67500.h5\n",
      "Epoch 27/150\n",
      "23/23 [==============================] - 105s 5s/step - loss: 0.5971 - categorical_accuracy: 0.9333 - val_loss: 1.0486 - val_categorical_accuracy: 0.6250\n",
      "\n",
      "Epoch 00027: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00027-0.59708-0.93333-1.04856-0.62500.h5\n",
      "Epoch 28/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.4339 - categorical_accuracy: 0.9362 - val_loss: 0.5688 - val_categorical_accuracy: 0.7083\n",
      "\n",
      "Epoch 00028: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00028-0.43385-0.93623-0.56881-0.70833.h5\n",
      "Epoch 29/150\n",
      "23/23 [==============================] - 106s 5s/step - loss: 0.4304 - categorical_accuracy: 0.9362 - val_loss: 0.5455 - val_categorical_accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00029: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00029-0.43040-0.93623-0.54554-0.75000.h5\n",
      "Epoch 30/150\n",
      "23/23 [==============================] - 106s 5s/step - loss: 0.4894 - categorical_accuracy: 0.9217 - val_loss: 0.8018 - val_categorical_accuracy: 0.7167\n",
      "\n",
      "Epoch 00030: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00030-0.48942-0.92174-0.80179-0.71667.h5\n",
      "Epoch 31/150\n",
      "23/23 [==============================] - 105s 5s/step - loss: 0.5054 - categorical_accuracy: 0.9246 - val_loss: 0.7130 - val_categorical_accuracy: 0.7250\n",
      "\n",
      "Epoch 00031: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00031-0.50541-0.92464-0.71298-0.72500.h5\n",
      "Epoch 32/150\n",
      "23/23 [==============================] - 106s 5s/step - loss: 0.6016 - categorical_accuracy: 0.9290 - val_loss: 1.2018 - val_categorical_accuracy: 0.6417\n",
      "\n",
      "Epoch 00032: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00032-0.60159-0.92899-1.20180-0.64167.h5\n",
      "Epoch 33/150\n",
      "23/23 [==============================] - 106s 5s/step - loss: 0.6042 - categorical_accuracy: 0.9246 - val_loss: 0.9017 - val_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00033: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00033-0.60416-0.92464-0.90171-0.67500.h5\n",
      "Epoch 34/150\n",
      "23/23 [==============================] - 106s 5s/step - loss: 0.6024 - categorical_accuracy: 0.9290 - val_loss: 0.7662 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00034: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00034-0.60242-0.92899-0.76622-0.70000.h5\n",
      "Epoch 35/150\n",
      "23/23 [==============================] - 106s 5s/step - loss: 0.6575 - categorical_accuracy: 0.9000 - val_loss: 1.1029 - val_categorical_accuracy: 0.6917\n",
      "\n",
      "Epoch 00035: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00035-0.65751-0.90000-1.10290-0.69167.h5\n",
      "Epoch 36/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 0.7944 - categorical_accuracy: 0.9145 - val_loss: 1.1694 - val_categorical_accuracy: 0.6833\n",
      "\n",
      "Epoch 00036: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00036-0.79436-0.91449-1.16940-0.68333.h5\n",
      "Epoch 37/150\n",
      "23/23 [==============================] - 107s 5s/step - loss: 0.7969 - categorical_accuracy: 0.9174 - val_loss: 1.0220 - val_categorical_accuracy: 0.7167\n",
      "\n",
      "Epoch 00037: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00037-0.79687-0.91739-1.02199-0.71667.h5\n",
      "Epoch 38/150\n",
      "23/23 [==============================] - 105s 5s/step - loss: 0.8264 - categorical_accuracy: 0.9232 - val_loss: 0.9947 - val_categorical_accuracy: 0.7167\n",
      "\n",
      "Epoch 00038: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00038-0.82643-0.92319-0.99472-0.71667.h5\n",
      "Epoch 39/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.6054 - categorical_accuracy: 0.9493 - val_loss: 0.6288 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00039: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00039-0.60543-0.94928-0.62878-0.77500.h5\n",
      "Epoch 40/150\n",
      "23/23 [==============================] - 102s 4s/step - loss: 0.3879 - categorical_accuracy: 0.9435 - val_loss: 0.4626 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00040: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00040-0.38795-0.94348-0.46264-0.77500.h5\n",
      "Epoch 41/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 0.3793 - categorical_accuracy: 0.9493 - val_loss: 0.5362 - val_categorical_accuracy: 0.7417\n",
      "\n",
      "Epoch 00041: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00041-0.37926-0.94928-0.53621-0.74167.h5\n",
      "Epoch 42/150\n",
      "23/23 [==============================] - 105s 5s/step - loss: 0.4055 - categorical_accuracy: 0.9377 - val_loss: 0.6089 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00042: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00042-0.40551-0.93768-0.60887-0.75000.h5\n",
      "Epoch 43/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 0.5224 - categorical_accuracy: 0.9362 - val_loss: 0.9009 - val_categorical_accuracy: 0.7167\n",
      "\n",
      "Epoch 00043: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00043-0.52235-0.93623-0.90091-0.71667.h5\n",
      "Epoch 44/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 0.7536 - categorical_accuracy: 0.9290 - val_loss: 1.2880 - val_categorical_accuracy: 0.7167\n",
      "\n",
      "Epoch 00044: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00044-0.75359-0.92899-1.28799-0.71667.h5\n",
      "Epoch 45/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.8488 - categorical_accuracy: 0.9159 - val_loss: 1.0450 - val_categorical_accuracy: 0.7333\n",
      "\n",
      "Epoch 00045: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00045-0.84880-0.91594-1.04502-0.73333.h5\n",
      "Epoch 46/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 0.7522 - categorical_accuracy: 0.9348 - val_loss: 1.0950 - val_categorical_accuracy: 0.7167\n",
      "\n",
      "Epoch 00046: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00046-0.75223-0.93478-1.09504-0.71667.h5\n",
      "Epoch 47/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.7272 - categorical_accuracy: 0.9391 - val_loss: 0.9543 - val_categorical_accuracy: 0.6917\n",
      "\n",
      "Epoch 00047: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00047-0.72723-0.93913-0.95428-0.69167.h5\n",
      "Epoch 48/150\n",
      "23/23 [==============================] - 105s 5s/step - loss: 0.6472 - categorical_accuracy: 0.9623 - val_loss: 0.8257 - val_categorical_accuracy: 0.7333\n",
      "\n",
      "Epoch 00048: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00048-0.64719-0.96232-0.82569-0.73333.h5\n",
      "Epoch 49/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.4016 - categorical_accuracy: 0.9565 - val_loss: 0.6035 - val_categorical_accuracy: 0.7250\n",
      "\n",
      "Epoch 00049: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00049-0.40158-0.95652-0.60354-0.72500.h5\n",
      "Epoch 50/150\n",
      "23/23 [==============================] - 105s 5s/step - loss: 0.2329 - categorical_accuracy: 0.9580 - val_loss: 0.3625 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00050: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00050-0.23286-0.95797-0.36249-0.77500.h5\n",
      "Epoch 51/150\n",
      "23/23 [==============================] - 102s 4s/step - loss: 0.2051 - categorical_accuracy: 0.9536 - val_loss: 0.3840 - val_categorical_accuracy: 0.7583\n",
      "\n",
      "Epoch 00051: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00051-0.20514-0.95362-0.38405-0.75833.h5\n",
      "Epoch 52/150\n",
      "23/23 [==============================] - 102s 4s/step - loss: 0.2174 - categorical_accuracy: 0.9536 - val_loss: 0.3673 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00052: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00052-0.21737-0.95362-0.36731-0.76667.h5\n",
      "Epoch 53/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.2587 - categorical_accuracy: 0.9507 - val_loss: 0.6613 - val_categorical_accuracy: 0.7250\n",
      "\n",
      "Epoch 00053: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00053-0.25868-0.95072-0.66128-0.72500.h5\n",
      "Epoch 54/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 0.3858 - categorical_accuracy: 0.9377 - val_loss: 0.7177 - val_categorical_accuracy: 0.7083\n",
      "\n",
      "Epoch 00054: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00054-0.38584-0.93768-0.71772-0.70833.h5\n",
      "Epoch 55/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 0.6848 - categorical_accuracy: 0.9203 - val_loss: 1.2309 - val_categorical_accuracy: 0.6917\n",
      "\n",
      "Epoch 00055: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00055-0.68477-0.92029-1.23087-0.69167.h5\n",
      "Epoch 56/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.9567 - categorical_accuracy: 0.9145 - val_loss: 1.2921 - val_categorical_accuracy: 0.7333\n",
      "\n",
      "Epoch 00056: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00056-0.95673-0.91449-1.29209-0.73333.h5\n",
      "Epoch 57/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 1.0888 - categorical_accuracy: 0.9188 - val_loss: 1.5119 - val_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00057: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00057-1.08879-0.91884-1.51191-0.67500.h5\n",
      "Epoch 58/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 103s 4s/step - loss: 0.9996 - categorical_accuracy: 0.9246 - val_loss: 1.1175 - val_categorical_accuracy: 0.7583\n",
      "\n",
      "Epoch 00058: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00058-0.99957-0.92464-1.11754-0.75833.h5\n",
      "Epoch 59/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.9179 - categorical_accuracy: 0.9290 - val_loss: 0.9971 - val_categorical_accuracy: 0.7417\n",
      "\n",
      "Epoch 00059: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00059-0.91793-0.92899-0.99714-0.74167.h5\n",
      "Epoch 60/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.8722 - categorical_accuracy: 0.9246 - val_loss: 1.3949 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00060: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00060-0.87222-0.92464-1.39494-0.70000.h5\n",
      "Epoch 61/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.9929 - categorical_accuracy: 0.9130 - val_loss: 1.0836 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00061: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00061-0.99288-0.91304-1.08355-0.75000.h5\n",
      "Epoch 62/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 0.8097 - categorical_accuracy: 0.9391 - val_loss: 1.1718 - val_categorical_accuracy: 0.7250\n",
      "\n",
      "Epoch 00062: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00062-0.80974-0.93913-1.17180-0.72500.h5\n",
      "Epoch 63/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 0.5515 - categorical_accuracy: 0.9507 - val_loss: 0.7434 - val_categorical_accuracy: 0.7167\n",
      "\n",
      "Epoch 00063: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00063-0.55145-0.95072-0.74338-0.71667.h5\n",
      "Epoch 64/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.4022 - categorical_accuracy: 0.9522 - val_loss: 0.5892 - val_categorical_accuracy: 0.7333\n",
      "\n",
      "Epoch 00064: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00064-0.40221-0.95217-0.58917-0.73333.h5\n",
      "Epoch 65/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.3305 - categorical_accuracy: 0.9551 - val_loss: 0.5887 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00065: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00065-0.33052-0.95507-0.58870-0.75000.h5\n",
      "Epoch 66/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 0.2282 - categorical_accuracy: 0.9681 - val_loss: 0.4405 - val_categorical_accuracy: 0.7417\n",
      "\n",
      "Epoch 00066: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00066-0.22822-0.96812-0.44048-0.74167.h5\n",
      "Epoch 67/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 0.1671 - categorical_accuracy: 0.9580 - val_loss: 0.3669 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00067: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00067-0.16714-0.95797-0.36695-0.75000.h5\n",
      "Epoch 68/150\n",
      "23/23 [==============================] - 105s 5s/step - loss: 0.2007 - categorical_accuracy: 0.9565 - val_loss: 0.4507 - val_categorical_accuracy: 0.7333\n",
      "\n",
      "Epoch 00068: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00068-0.20073-0.95652-0.45066-0.73333.h5\n",
      "Epoch 69/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.2709 - categorical_accuracy: 0.9507 - val_loss: 0.6360 - val_categorical_accuracy: 0.7250\n",
      "\n",
      "Epoch 00069: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00069-0.27086-0.95072-0.63602-0.72500.h5\n",
      "Epoch 70/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.2891 - categorical_accuracy: 0.9507 - val_loss: 1.0920 - val_categorical_accuracy: 0.6500\n",
      "\n",
      "Epoch 00070: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00070-0.28914-0.95072-1.09195-0.65000.h5\n",
      "Epoch 71/150\n",
      "23/23 [==============================] - 105s 5s/step - loss: 0.6078 - categorical_accuracy: 0.9304 - val_loss: 1.2874 - val_categorical_accuracy: 0.6833\n",
      "\n",
      "Epoch 00071: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00071-0.60778-0.93043-1.28743-0.68333.h5\n",
      "Epoch 72/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.7385 - categorical_accuracy: 0.9348 - val_loss: 1.2831 - val_categorical_accuracy: 0.7167\n",
      "\n",
      "Epoch 00072: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00072-0.73855-0.93478-1.28310-0.71667.h5\n",
      "Epoch 73/150\n",
      "23/23 [==============================] - 102s 4s/step - loss: 0.9043 - categorical_accuracy: 0.9464 - val_loss: 1.5953 - val_categorical_accuracy: 0.6917\n",
      "\n",
      "Epoch 00073: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00073-0.90431-0.94638-1.59530-0.69167.h5\n",
      "Epoch 74/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 1.1165 - categorical_accuracy: 0.9304 - val_loss: 1.5456 - val_categorical_accuracy: 0.7333\n",
      "\n",
      "Epoch 00074: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00074-1.11651-0.93043-1.54563-0.73333.h5\n",
      "Epoch 75/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 1.1185 - categorical_accuracy: 0.9391 - val_loss: 1.1389 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00075: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00075-1.11848-0.93913-1.13889-0.76667.h5\n",
      "Epoch 76/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 0.8324 - categorical_accuracy: 0.9362 - val_loss: 1.1884 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00076: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00076-0.83237-0.93623-1.18840-0.70000.h5\n",
      "Epoch 77/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.6223 - categorical_accuracy: 0.9551 - val_loss: 0.6493 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00077: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00077-0.62229-0.95507-0.64930-0.76667.h5\n",
      "Epoch 78/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.4908 - categorical_accuracy: 0.9493 - val_loss: 0.6397 - val_categorical_accuracy: 0.7917\n",
      "\n",
      "Epoch 00078: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00078-0.49078-0.94928-0.63971-0.79167.h5\n",
      "Epoch 79/150\n",
      "23/23 [==============================] - 106s 5s/step - loss: 0.3505 - categorical_accuracy: 0.9536 - val_loss: 0.5019 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00079: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00079-0.35054-0.95362-0.50186-0.76667.h5\n",
      "Epoch 80/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 0.3091 - categorical_accuracy: 0.9507 - val_loss: 0.4782 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00080: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00080-0.30908-0.95072-0.47816-0.77500.h5\n",
      "Epoch 81/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 0.2418 - categorical_accuracy: 0.9594 - val_loss: 0.3129 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00081: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00081-0.24178-0.95942-0.31286-0.77500.h5\n",
      "Epoch 82/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.1834 - categorical_accuracy: 0.9580 - val_loss: 0.4237 - val_categorical_accuracy: 0.7583\n",
      "\n",
      "Epoch 00082: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00082-0.18337-0.95797-0.42374-0.75833.h5\n",
      "Epoch 83/150\n",
      "23/23 [==============================] - 105s 5s/step - loss: 0.1792 - categorical_accuracy: 0.9594 - val_loss: 0.3231 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00083: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00083-0.17923-0.95942-0.32307-0.76667.h5\n",
      "Epoch 84/150\n",
      "23/23 [==============================] - 105s 5s/step - loss: 0.1622 - categorical_accuracy: 0.9551 - val_loss: 0.3727 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00084: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00084-0.16215-0.95507-0.37268-0.76667.h5\n",
      "Epoch 85/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 0.2145 - categorical_accuracy: 0.9855 - val_loss: 0.6320 - val_categorical_accuracy: 0.7333\n",
      "\n",
      "Epoch 00085: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00085-0.21451-0.98551-0.63196-0.73333.h5\n",
      "Epoch 86/150\n",
      "23/23 [==============================] - 105s 5s/step - loss: 0.2301 - categorical_accuracy: 0.9580 - val_loss: 0.5566 - val_categorical_accuracy: 0.7667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00086: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00086-0.23007-0.95797-0.55661-0.76667.h5\n",
      "Epoch 87/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 0.5949 - categorical_accuracy: 0.9319 - val_loss: 1.5386 - val_categorical_accuracy: 0.6583\n",
      "\n",
      "Epoch 00087: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00087-0.59486-0.93188-1.53865-0.65833.h5\n",
      "Epoch 88/150\n",
      "23/23 [==============================] - 102s 4s/step - loss: 1.6806 - categorical_accuracy: 0.9087 - val_loss: 2.3724 - val_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00088: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00088-1.68059-0.90870-2.37245-0.67500.h5\n",
      "Epoch 89/150\n",
      "23/23 [==============================] - 105s 5s/step - loss: 2.1963 - categorical_accuracy: 0.9145 - val_loss: 2.2568 - val_categorical_accuracy: 0.7333\n",
      "\n",
      "Epoch 00089: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00089-2.19632-0.91449-2.25675-0.73333.h5\n",
      "Epoch 90/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 1.5646 - categorical_accuracy: 0.9261 - val_loss: 1.6517 - val_categorical_accuracy: 0.7417\n",
      "\n",
      "Epoch 00090: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00090-1.56460-0.92609-1.65167-0.74167.h5\n",
      "Epoch 91/150\n",
      "23/23 [==============================] - 102s 4s/step - loss: 1.3294 - categorical_accuracy: 0.9261 - val_loss: 1.7524 - val_categorical_accuracy: 0.7167\n",
      "\n",
      "Epoch 00091: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00091-1.32944-0.92609-1.75235-0.71667.h5\n",
      "Epoch 92/150\n",
      "23/23 [==============================] - 105s 5s/step - loss: 1.2781 - categorical_accuracy: 0.9449 - val_loss: 1.4973 - val_categorical_accuracy: 0.7167\n",
      "\n",
      "Epoch 00092: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00092-1.27807-0.94493-1.49726-0.71667.h5\n",
      "Epoch 93/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 0.8203 - categorical_accuracy: 0.9507 - val_loss: 0.8209 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00093: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00093-0.82034-0.95072-0.82093-0.76667.h5\n",
      "Epoch 94/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 0.4049 - categorical_accuracy: 0.9580 - val_loss: 0.4458 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00094: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00094-0.40488-0.95797-0.44580-0.76667.h5\n",
      "Epoch 95/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.2204 - categorical_accuracy: 0.9594 - val_loss: 0.3796 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00095: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00095-0.22036-0.95942-0.37956-0.77500.h5\n",
      "Epoch 96/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.1591 - categorical_accuracy: 0.9580 - val_loss: 0.2730 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00096: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00096-0.15908-0.95797-0.27302-0.80000.h5\n",
      "Epoch 97/150\n",
      "23/23 [==============================] - 102s 4s/step - loss: 0.1837 - categorical_accuracy: 0.9551 - val_loss: 0.3407 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00097: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00097-0.18372-0.95507-0.34066-0.80000.h5\n",
      "Epoch 98/150\n",
      "23/23 [==============================] - 102s 4s/step - loss: 0.2212 - categorical_accuracy: 0.9551 - val_loss: 0.4323 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00098: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00098-0.22119-0.95507-0.43226-0.75000.h5\n",
      "Epoch 99/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.2193 - categorical_accuracy: 0.9551 - val_loss: 0.4087 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00099: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00099-0.21934-0.95507-0.40875-0.76667.h5\n",
      "Epoch 100/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.2221 - categorical_accuracy: 0.9565 - val_loss: 0.3467 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00100: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00100-0.22209-0.95652-0.34672-0.77500.h5\n",
      "Epoch 101/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.2156 - categorical_accuracy: 0.9580 - val_loss: 0.3601 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00101: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00101-0.21555-0.95797-0.36010-0.77500.h5\n",
      "Epoch 102/150\n",
      "23/23 [==============================] - 105s 5s/step - loss: 0.2334 - categorical_accuracy: 0.9551 - val_loss: 0.3850 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00102: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00102-0.23340-0.95507-0.38504-0.76667.h5\n",
      "Epoch 103/150\n",
      "23/23 [==============================] - 102s 4s/step - loss: 0.2309 - categorical_accuracy: 0.9565 - val_loss: 0.4346 - val_categorical_accuracy: 0.7833\n",
      "\n",
      "Epoch 00103: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00103-0.23090-0.95652-0.43460-0.78333.h5\n",
      "Epoch 104/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 0.5442 - categorical_accuracy: 0.9681 - val_loss: 1.0262 - val_categorical_accuracy: 0.7417\n",
      "\n",
      "Epoch 00104: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00104-0.54419-0.96812-1.02621-0.74167.h5\n",
      "Epoch 105/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 1.1000 - categorical_accuracy: 0.9449 - val_loss: 2.5234 - val_categorical_accuracy: 0.6417\n",
      "\n",
      "Epoch 00105: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00105-1.09998-0.94493-2.52341-0.64167.h5\n",
      "Epoch 106/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 2.8237 - categorical_accuracy: 0.9116 - val_loss: 3.2656 - val_categorical_accuracy: 0.7583\n",
      "\n",
      "Epoch 00106: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00106-2.82369-0.91159-3.26556-0.75833.h5\n",
      "Epoch 107/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 2.5804 - categorical_accuracy: 0.9130 - val_loss: 2.7783 - val_categorical_accuracy: 0.6417\n",
      "\n",
      "Epoch 00107: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00107-2.58040-0.91304-2.77829-0.64167.h5\n",
      "Epoch 108/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 1.5182 - categorical_accuracy: 0.9406 - val_loss: 1.4040 - val_categorical_accuracy: 0.7417\n",
      "\n",
      "Epoch 00108: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00108-1.51818-0.94058-1.40404-0.74167.h5\n",
      "Epoch 109/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 0.7218 - categorical_accuracy: 0.9565 - val_loss: 0.6374 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00109: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00109-0.72180-0.95652-0.63738-0.77500.h5\n",
      "Epoch 110/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.3163 - categorical_accuracy: 0.9580 - val_loss: 0.4438 - val_categorical_accuracy: 0.7833\n",
      "\n",
      "Epoch 00110: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00110-0.31631-0.95797-0.44384-0.78333.h5\n",
      "Epoch 111/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.2204 - categorical_accuracy: 0.9594 - val_loss: 0.3546 - val_categorical_accuracy: 0.7917\n",
      "\n",
      "Epoch 00111: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00111-0.22044-0.95942-0.35456-0.79167.h5\n",
      "Epoch 112/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 0.1517 - categorical_accuracy: 0.9870 - val_loss: 0.2509 - val_categorical_accuracy: 0.9667\n",
      "\n",
      "Epoch 00112: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00112-0.15172-0.98696-0.25089-0.96667.h5\n",
      "Epoch 113/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.1160 - categorical_accuracy: 0.9594 - val_loss: 0.2348 - val_categorical_accuracy: 0.7917\n",
      "\n",
      "Epoch 00113: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00113-0.11602-0.95942-0.23476-0.79167.h5\n",
      "Epoch 114/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.1084 - categorical_accuracy: 0.9609 - val_loss: 0.2572 - val_categorical_accuracy: 0.7917\n",
      "\n",
      "Epoch 00114: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00114-0.10842-0.96087-0.25718-0.79167.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/150\n",
      "23/23 [==============================] - 102s 4s/step - loss: 0.0966 - categorical_accuracy: 0.9594 - val_loss: 0.1978 - val_categorical_accuracy: 0.7833\n",
      "\n",
      "Epoch 00115: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00115-0.09664-0.95942-0.19780-0.78333.h5\n",
      "Epoch 116/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 0.1139 - categorical_accuracy: 0.9609 - val_loss: 0.2692 - val_categorical_accuracy: 0.7833\n",
      "\n",
      "Epoch 00116: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00116-0.11387-0.96087-0.26923-0.78333.h5\n",
      "Epoch 117/150\n",
      "23/23 [==============================] - 105s 5s/step - loss: 0.0997 - categorical_accuracy: 0.9609 - val_loss: 0.2023 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00117: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00117-0.09972-0.96087-0.20233-0.80000.h5\n",
      "Epoch 118/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 0.0808 - categorical_accuracy: 0.9739 - val_loss: 0.1696 - val_categorical_accuracy: 0.8083\n",
      "\n",
      "Epoch 00118: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00118-0.08083-0.97391-0.16963-0.80833.h5\n",
      "Epoch 119/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.0724 - categorical_accuracy: 0.9609 - val_loss: 0.1353 - val_categorical_accuracy: 0.8083\n",
      "\n",
      "Epoch 00119: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00119-0.07240-0.96087-0.13532-0.80833.h5\n",
      "Epoch 120/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 0.0656 - categorical_accuracy: 0.9609 - val_loss: 0.1718 - val_categorical_accuracy: 0.9750\n",
      "\n",
      "Epoch 00120: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00120-0.06563-0.96087-0.17184-0.97500.h5\n",
      "Epoch 121/150\n",
      "23/23 [==============================] - 102s 4s/step - loss: 0.0572 - categorical_accuracy: 0.9594 - val_loss: 0.1807 - val_categorical_accuracy: 0.9750\n",
      "\n",
      "Epoch 00121: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00121-0.05721-0.95942-0.18074-0.97500.h5\n",
      "Epoch 122/150\n",
      "23/23 [==============================] - 102s 4s/step - loss: 0.1955 - categorical_accuracy: 0.9449 - val_loss: 1.0157 - val_categorical_accuracy: 0.7333\n",
      "\n",
      "Epoch 00122: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00122-0.19549-0.94493-1.01571-0.73333.h5\n",
      "Epoch 123/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 1.2946 - categorical_accuracy: 0.9000 - val_loss: 3.5546 - val_categorical_accuracy: 0.5833\n",
      "\n",
      "Epoch 00123: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00123-1.29462-0.90000-3.55461-0.58333.h5\n",
      "Epoch 124/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 3.9256 - categorical_accuracy: 0.8884 - val_loss: 4.1945 - val_categorical_accuracy: 0.7417\n",
      "\n",
      "Epoch 00124: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00124-3.92559-0.88841-4.19448-0.74167.h5\n",
      "Epoch 125/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 3.1457 - categorical_accuracy: 0.9043 - val_loss: 3.1452 - val_categorical_accuracy: 0.7167\n",
      "\n",
      "Epoch 00125: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00125-3.14571-0.90435-3.14518-0.71667.h5\n",
      "Epoch 126/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 2.1291 - categorical_accuracy: 0.9159 - val_loss: 2.0943 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00126: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00126-2.12911-0.91594-2.09432-0.77500.h5\n",
      "Epoch 127/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 1.3225 - categorical_accuracy: 0.9420 - val_loss: 1.5847 - val_categorical_accuracy: 0.7333\n",
      "\n",
      "Epoch 00127: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00127-1.32252-0.94203-1.58469-0.73333.h5\n",
      "Epoch 128/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 1.1322 - categorical_accuracy: 0.9377 - val_loss: 1.3462 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00128: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00128-1.13220-0.93768-1.34616-0.75000.h5\n",
      "Epoch 129/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.6930 - categorical_accuracy: 0.9884 - val_loss: 0.7027 - val_categorical_accuracy: 0.9167\n",
      "\n",
      "Epoch 00129: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00129-0.69299-0.98841-0.70269-0.91667.h5\n",
      "Epoch 130/150\n",
      "23/23 [==============================] - 105s 5s/step - loss: 0.3302 - categorical_accuracy: 0.9609 - val_loss: 0.4554 - val_categorical_accuracy: 0.7833\n",
      "\n",
      "Epoch 00130: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00130-0.33020-0.96087-0.45545-0.78333.h5\n",
      "Epoch 131/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.1846 - categorical_accuracy: 0.9609 - val_loss: 0.3576 - val_categorical_accuracy: 0.7833\n",
      "\n",
      "Epoch 00131: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00131-0.18457-0.96087-0.35760-0.78333.h5\n",
      "Epoch 132/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 0.1447 - categorical_accuracy: 0.9536 - val_loss: 0.3568 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00132: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00132-0.14469-0.95362-0.35682-0.77500.h5\n",
      "Epoch 133/150\n",
      "23/23 [==============================] - 105s 5s/step - loss: 0.1553 - categorical_accuracy: 0.9565 - val_loss: 0.3948 - val_categorical_accuracy: 0.7583\n",
      "\n",
      "Epoch 00133: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00133-0.15526-0.95652-0.39482-0.75833.h5\n",
      "Epoch 134/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.1989 - categorical_accuracy: 0.9580 - val_loss: 0.4311 - val_categorical_accuracy: 0.7583\n",
      "\n",
      "Epoch 00134: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00134-0.19892-0.95797-0.43108-0.75833.h5\n",
      "Epoch 135/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.2133 - categorical_accuracy: 0.9478 - val_loss: 0.3788 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00135: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00135-0.21335-0.94783-0.37878-0.76667.h5\n",
      "Epoch 136/150\n",
      "23/23 [==============================] - 105s 5s/step - loss: 0.2454 - categorical_accuracy: 0.9493 - val_loss: 0.4967 - val_categorical_accuracy: 0.7750\n",
      "\n",
      "Epoch 00136: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00136-0.24538-0.94928-0.49669-0.77500.h5\n",
      "Epoch 137/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 0.2960 - categorical_accuracy: 0.9580 - val_loss: 0.3999 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00137: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00137-0.29601-0.95797-0.39988-0.76667.h5\n",
      "Epoch 138/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 0.2163 - categorical_accuracy: 0.9594 - val_loss: 0.2990 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00138: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00138-0.21627-0.95942-0.29896-0.80000.h5\n",
      "Epoch 139/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.1268 - categorical_accuracy: 0.9609 - val_loss: 0.2538 - val_categorical_accuracy: 0.7833\n",
      "\n",
      "Epoch 00139: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00139-0.12679-0.96087-0.25379-0.78333.h5\n",
      "Epoch 140/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.0953 - categorical_accuracy: 0.9609 - val_loss: 0.1726 - val_categorical_accuracy: 0.7833\n",
      "\n",
      "Epoch 00140: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00140-0.09530-0.96087-0.17263-0.78333.h5\n",
      "Epoch 141/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 0.0919 - categorical_accuracy: 0.9594 - val_loss: 0.1930 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00141: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00141-0.09190-0.95942-0.19304-0.76667.h5\n",
      "Epoch 142/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.1410 - categorical_accuracy: 0.9551 - val_loss: 0.3057 - val_categorical_accuracy: 0.7583\n",
      "\n",
      "Epoch 00142: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00142-0.14096-0.95507-0.30565-0.75833.h5\n",
      "Epoch 143/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.3302 - categorical_accuracy: 0.9464 - val_loss: 0.5410 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00143: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00143-0.33019-0.94638-0.54098-0.75000.h5\n",
      "Epoch 144/150\n",
      "23/23 [==============================] - 102s 4s/step - loss: 0.8985 - categorical_accuracy: 0.9319 - val_loss: 2.2559 - val_categorical_accuracy: 0.6667\n",
      "\n",
      "Epoch 00144: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00144-0.89853-0.93188-2.25592-0.66667.h5\n",
      "Epoch 145/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 2.7195 - categorical_accuracy: 0.8841 - val_loss: 3.9992 - val_categorical_accuracy: 0.6583\n",
      "\n",
      "Epoch 00145: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00145-2.71952-0.88406-3.99919-0.65833.h5\n",
      "Epoch 146/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 3.7135 - categorical_accuracy: 0.9014 - val_loss: 3.7498 - val_categorical_accuracy: 0.8917\n",
      "\n",
      "Epoch 00146: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00146-3.71352-0.90145-3.74977-0.89167.h5\n",
      "Epoch 147/150\n",
      "23/23 [==============================] - 102s 4s/step - loss: 2.4579 - categorical_accuracy: 0.9188 - val_loss: 2.2124 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00147: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00147-2.45790-0.91884-2.21237-0.76667.h5\n",
      "Epoch 148/150\n",
      "23/23 [==============================] - 102s 4s/step - loss: 1.4718 - categorical_accuracy: 0.9594 - val_loss: 1.5249 - val_categorical_accuracy: 0.7167\n",
      "\n",
      "Epoch 00148: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00148-1.47177-0.95942-1.52491-0.71667.h5\n",
      "Epoch 149/150\n",
      "23/23 [==============================] - 104s 5s/step - loss: 0.9387 - categorical_accuracy: 0.9362 - val_loss: 1.1678 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00149: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00149-0.93867-0.93623-1.16776-0.75000.h5\n",
      "Epoch 150/150\n",
      "23/23 [==============================] - 103s 4s/step - loss: 0.6327 - categorical_accuracy: 0.9565 - val_loss: 0.6667 - val_categorical_accuracy: 0.7667\n",
      "\n",
      "Epoch 00150: saving model to model_init_CNN_RNN_2019-03-1518_12_18.422945/model-00150-0.63272-0.95652-0.66670-0.76667.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a09a247b00>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Accuracy at epoch: 120\n",
    "#### Train Acc: 9609\n",
    "#### Validation Acc: 9750"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, from the results obtained above we conclude the 3D Convolutional model to be the best and final model, as it gives the highest accuracy score and has fewer number of model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
